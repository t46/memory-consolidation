\documentclass[12pt]{article}

\usepackage{bm}
\usepackage{amsfonts}
\usepackage{mathtools}

\begin{document}

\title{Hypothesis}
\author{Shiro Takagi}
\date{2021/4/28}
\maketitle

\section{Preface}

\subsection{Problem setting}
We consider the distributed representation. A memory of experience $\bm{x} \in X \subset \mathbb{R}^n$
is a function $f: X \mapsto \Theta$. For simplicity, we denote a memory by $\bm{\theta} \in \Theta \subset \mathbb{R}^p$. 
Note that $n$ and $p$ are dimension of vector $\bm{x}$ and $\bm{\theta}$.

We discuss short term memory and long term memory. Short term memory is a function 
$f^s: X \mapsto \Theta^s$, where $\bm{\theta}^s \in \Theta^s \subset \mathbb{R}^{p^s}$. 
Long term memory is a compositional function $f^l \coloneqq f^c \circ f^s$, where $f^c: \Theta^s \mapsto \Theta^l$ 
is a memory consolidation function and $\bm{\theta}^l \in \Theta^l \subset \mathbb{R}^{p^l}$. Also, we consider 
memory retrieval function $f^r: \Theta^l \mapsto \Theta^s$. We assume that 
$p^l$ is finite and fixed

\subsection{Goal}
Our goal is to find optimal memory consolidation function $f^c$ and 
memory retrieval function $f^r$, given some criterion. 
Followings are what we think are desirable properties for these functions to have:
\begin{itemize}
    \item long term memory retains short term memory's information as much as possible 
    \item long term memory is retrievable by memory retrieval function
    \item memory retrieval function retrieves stored information as much as possible
\end{itemize} 
In a nut shell, we want the functions such that 
\begin{equation}
    \forall \varepsilon, || f^r(f^l(\bm{x})) - \bm{x} || < \varepsilon.
\end{equation}
The left hand side of the equation is just a reconstruction loss.

Another thing to consider is how to combine long term memory to short term memory. 
Humans seem to elegantly and naturally exploit these two memory to do a task. Thus, 
long term memory should be encoded and decoded such that it can be exploited easily. 

\subsection{Memory representation}
We represent memory as just a vector with no structure $\bm{\theta}$. However, 
the relation between each component of a distributed representation are generally asymmetric. 
For example, parameters of fully connected neural network construct a hierarchical structure. 
Therefore, considering an optimal structure of a distributed representation, given 
some criterion, is another issue to consider. 

Bunch of studies discussed how to construct parameters for to do tasks well (short term memory). 
My aim is to elucidate an optimal structure for long term memory. 

\subsection{Experience representation}
We should also consider how to represent experience. For supervised learning, experience may be 
a tuple of data, loss function, and algorithm. For reinforcement learning, experience may be a tuple 
of state, action, and loss (reword) function. A formal definition of task by Finn et al. is 
helpful to consider this issue \cite{Finn17}. 

If we include loss function and algorithm in the definition of experience, it might not be 
suitable to call $\bm{x}$ experience and $\phi$ memory, because loss function and algorithm 
are usually included in $\phi$ and $\bm{x}$ is data.

We define experience at a time $t$ as a tuple of following components:
\begin{itemize}
    \item observation: $\bm{s}_t \in \mathcal{S}$,
    \item next observation: $\bm{s}_{t+1} \in \mathcal{S}$,
    \item action: $\bm{a}_t \in \mathcal{A}$,
\end{itemize}
where $\mathcal{S}$ is state space and $\mathcal{A}$ is action space. 
A state is a function from a pair of state and an action:
\begin{equation}
    s_t: \mathcal{S} \times \mathcal{A} \mapsto \mathcal{S}
\end{equation}
An action is a function from memory and state to action space:
\begin{equation}
    a_t: \mathcal{S} \times \Theta \mapsto \mathcal{A}
\end{equation}
We assume that memory consists of only short term and long term memory:
\begin{equation}
    \Theta = \Theta^s \cup \Theta^l, \Theta^s \cap \Theta^l = \emptyset
\end{equation}

Therefore, $\bm{x}_t = (\bm{s}_t, \bm{a}_t)$.

\section{Survey}
\subsection{Method}
I read survey papers such as \cite{Parisi19} and pick up the literatures I think is important. 

\subsection{Human}
Human brains is though to have complementary learning system, where hippocampus is for 
fast learning and neocortex is for slow learning \cite{Mcclelland95}. \textit{``More specifically, the hippocampus employs
a rapid learning rate and encodes sparse representations of events to minimize interference. Conversely, the neocortex is characterized by a slow learning rate and builds overlapping representations
of the learned knowledge''} \cite{Parisi19}. It matters that ``the hippocampus is thought to represent experiences in pattern separated fashion, 
whereby in the idealized case even highly similar events are allocated
neuronal codes that are non-overlapping or orthogonal'' \cite{Kumaran16}.

They do not think that 
\textit{``the hippocampal system receives a direct copy of
the pattern of activation distributed over the higher level regions
of the neocortical system; instead, the neocortical representation is thought to be re-represented in a compressed format over
a much smaller number of neurons in the hippocampal system'' }\cite{Mcclelland95}. The point is that 
``such compression can often occur without loss of essential 
information if there is redundancy in the neocortical representations" \cite{Mcclelland95} 
\footnote{Why compression? How to compress information?}. They say \textit{``it (hippocampus) can be
viewed not just as a memory store but as the teacher of the neocortical processing system''} 
\cite{Mcclelland95}. In sum, \textit{``The temporally extended and
graded nature of retrograde amnesia would reflect the fact that
information initially stored in the hippocampal memory system can become incorporated into the neocortical system only
very gradually, as a result of the small size of the changes made
on each reinstatement''} \cite{Mcclelland95}. I think following statement crucial 

\begin{quote}
    \textit{One might then be tempted
    to suggest that McCloskey and Cohen simply used the wrong
    kind of representation and that the problem could be eliminated by using sparser patterns of activation with less overlap.
    However, as French (1991) has noted, reducing overlap avoids
    catastrophic interference at the cost of a dramatic reduction in
    the exploitation of shared structure --- However, the existence of hippocampal amnesia, together
    with the sketch given earlier of the possible role of the hippocampal system in learning and memory, suggests instead that
    one might use the success of Rumelhart's (1990) simulation,
    together with the failure of McCloskey and Cohen's (1989), as
    the basis for understanding why there is a separate learning system in the hippocampus and why knowledge originally stored
    in this system is incorporated in the neocortex only gradually.} \cite{Mcclelland95}    
\end{quote}

Following are particularly important answers to key questions presented in this literature
\begin{itemize}
    \item \textit{``The principles indicate that the hippocampus is there to provide a medium for the initial storage of memories in a form that
    avoids interference with the knowledge already acquired in the
    neocortical system''}
    \item \textit{``Incorporation takes a long time to allow new knowledge to be
    interleaved with ongoing exposure to exemplars of the existing
    knowledge structure, so that eventually the new knowledge may
    be incorporated into the structured system already contained
    in the neocortex. If the changes were made rapidly, they would
    interfere with the system of structured knowledge built up from
    prior experience with other related material.''} \footnote{Intuitive but why?}
\end{itemize}

Following are interesting interpretation of the relation between current artificial external memory 
and human memory system:
\begin{quote}
    \textit{While parallels have been drawn between the external memory of the NTM and working memory, the characteristics
    of its external memory can easily be related to long-term memory systems as well. Indeed, content-based addressable
    external memories of this kind share functionalities with attractor networks, an architecture often used to model the
    computational functions performed by the CA3 subregion of the hippocampus (e.g., storage and retrieval of episodic
    memories). There are further points of connection between the operation of the NTM and the hippocampus:
    information is not stored and retained indiscriminately; instead it is selected based on an estimate of potential future
    relevance (see section ‘Proposed Role for the Hippocampus in Circumventing the Statistics of the Environment’)} \cite{Kumaran16} 
\end{quote}

An interesting observation on how mammalian memory mitigate catastrophic 
forgetting:
\begin{quote}
    \textit{The distributed nature of neural coding can lead to interference between sensory and memory representations. 
    Here, we show that the brain mitigates such interference by rotating sensory representations into 
    orthogonal memory representations over time ... 
    The transformation of sensory information into a memory was facilitated by a combination of ‘stable’ neurons, 
    which maintained their selectivity over time, and ‘switching’ neurons, which inverted their selectivity over time. 
    Together, these neural responses rotated the population representation, transforming sensory inputs into memory.
    }\cite{Libby21}
\end{quote}

\subsection{Artificial neural network}
Dual-weight learning system have fast-learning weight and slow-learning weight \cite{Hinton87}. Pseudo-rehearsal 
does not explicitly store memory but store as probabilistic model \cite{Robins95}. Recent approach based on 
a similar idea is deep generative replay \cite{Shin17}. Note that this approach is inspired by the 
generative role of hippocampus not by neocortical function. Soltoggio et al. 
proposed hypothesis testing plasticity, in which confidence of consistency of cause-effect relationships 
determines if a memory is short-term or long-term \cite{Soltoggio15}. Lopez-Paz and Ranzato proposed 
Gradient Episodic Memory, which impose constraint that overlap between gradient of current task and 
old task is sufficiently large \cite{Lopez17}. Kamara et al. also model long term memory 
as generative model \cite{Kemker17,Kemker18} 
\footnote{Is generative model the best/only way to model long term memory? What is the functionality 
of this model}.

\subsection{Deep Generative Dual Memory Network}
Deep generative dual memory network comprises two generative models: 
a short-term memory (STM) and a long-term memory (LTM) \cite{Kamra17}. What I thought are 
following:
\begin{itemize}
    \item As conventions, it encodes task in memory. What should I encode? Is task the best thing to encode?
    \item Following Lopez-Paz et al. it measures performance with Average accuracy and Backward Transfer. Should I use them?
    \item They propose a balancing algorithm between old memory and new memory. But it looks heuristic. Is there another principled way to do this?
    \item Similarly, they propose a way to use their algorithm without task index. Is the proposed way the best way to do this?
    \item Benchmark task is too simple and not well-motivated. 
\end{itemize}

\section{Notes}
Lets's reconsider the problems of current memory system. My goal is to 
find an answer for a good memory consolidation algorithm. To that end, 
thinking about how to represent memory, or, what structure memory should 
be is important. My thoughts are here:
\begin{itemize}
    \item Memory should encode temporal information
    \item Memory should represents experience not task
    \item Ideal structure for learning/inference/task solving and that for 
          storage and retrieval is not necessarily the same; we should 
          separate these two requirements
    \item Memory is fragile but once consolidated, it is surprisingly robust
    \item (Conscious) memory is ``used'' to do something new
    \item Memory is associative
    \item Every association should be temporal
\end{itemize}



\bibliography{ref}
\bibliographystyle{plain}

\end{document}