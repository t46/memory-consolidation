\documentclass[12pt]{article}

\usepackage{bm}
\usepackage{amsfonts}
\usepackage{mathtools}

\begin{document}

\title{Hypothesis}
\author{Shiro Takagi}
\date{2021/4/28}
\maketitle

\section{Preface}

\subsection{Problem setting}
We consider the distributed representation. A memory of experience $\bm{x} \in X \subset \mathbb{R}^n$
is a function $f: X \mapsto \Theta$. For simplicity, we denote a memory by $\bm{\theta} \in \Theta \subset \mathbb{R}^p$. 
Note that $n$ and $p$ are dimension of vector $\bm{x}$ and $\bm{\theta}$.

We discuss short term memory and long term memory. Short term memory is a function 
$f^s: X \mapsto \Theta^s$, where $\bm{\theta}^s \in \Theta^s \subset \mathbb{R}^{p^s}$. 
Long term memory is a compositional function $f^l \coloneqq f^c \circ f^s$, where $f^c: \Theta^s \mapsto \Theta^l$ 
is a memory consolidation function and $\bm{\theta}^l \in \Theta^l \subset \mathbb{R}^{p^l}$. Also, we consider 
memory retrieval function $f^r: \Theta^l \mapsto \Theta^s$. We assume that 
$p^l$ is finite and fixed

\subsection{Goal}
Our goal is to find optimal memory consolidation function $f^c$ and 
memory retrieval function $f^r$, given some criterion. 
Followings are what we think are desirable properties for these functions to have:
\begin{itemize}
    \item long term memory retains short term memory's information as much as possible 
    \item long term memory is retrievable by memory retrieval function
    \item memory retrieval function retrieves stored information as much as possible
\end{itemize} 
In a nut shell, we want the functions such that 
\begin{equation}
    \forall \varepsilon, || f^r(f^l(\bm{x})) - \bm{x} || < \varepsilon.
\end{equation}
The left hand side of the equation is just a reconstruction loss.

Another thing to consider is how to combine long term memory to short term memory. 
Humans seem to elegantly and naturally exploit these two memory to do a task. Thus, 
long term memory should be encoded and decoded such that it can be exploited easily. 

\subsection{Memory representation}
We represent memory as just a vector with no structure $\bm{\theta}$. However, 
the relation between each component of a distributed representation are generally asymmetric. 
For example, parameters of fully connected neural network construct a hierarchical structure. 
Therefore, considering an optimal structure of a distributed representation, given 
some criterion, is another issue to consider. 

Bunch of studies discussed how to construct parameters for to do tasks well (short term memory). 
My aim is to elucidate an optimal structure for long term memory. 

\subsection{Experience representation}
We should also consider how to represent experience. For supervised learning, experience may be 
a tuple of data, loss function, and algorithm. For reinforcement learning, experience may be a tuple 
of state, action, and loss (reword) function. A formal definition of task by Finn et al. is 
helpful to consider this issue \cite{Finn17}. 

If we include loss function and algorithm in the definition of experience, it might not be 
suitable to call $\bm{x}$ experience and $\phi$ memory, because loss function and algorithm 
are usually included in $\phi$ and $\bm{x}$ is data.

We define experience at a time $t$ as a tuple of following components:
\begin{itemize}
    \item observation: $\bm{s}_t \in \mathcal{S}$,
    \item next observation: $\bm{s}_{t+1} \in \mathcal{S}$,
    \item action: $\bm{a}_t \in \mathcal{A}$,
\end{itemize}
where $\mathcal{S}$ is state space and $\mathcal{A}$ is action space. 
A state is a function from a pair of state and an action:
\begin{equation}
    s_t: \mathcal{S} \times \mathcal{A} \mapsto \mathcal{S}
\end{equation}
An action is a function from memory and state to action space:
\begin{equation}
    a_t: \mathcal{S} \times \Theta \mapsto \mathcal{A}
\end{equation}
We assume that memory consists of only short term and long term memory:
\begin{equation}
    \Theta = \Theta^s \cup \Theta^l, \Theta^s \cap \Theta^l = \emptyset
\end{equation}

Therefore, $\bm{x}_t = (\bm{s}_t, \bm{a}_t)$.






\bibliography{ref}
\bibliographystyle{plain}

\end{document}